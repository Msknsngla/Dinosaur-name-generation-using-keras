{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naming_Dinausaurs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eie-Y9feIBih"
      },
      "source": [
        "**Importing the required libraries.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h4i9HM-_xRA"
      },
      "source": [
        "import numpy as np\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM, Dense\r\n",
        "from keras.callbacks import LambdaCallback"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktCAfCEtIqe3"
      },
      "source": [
        "**Creating the vocabulary** <br>\r\n",
        "We read the data from the text file and split it into a list of uniques charachters which will be used as the dictionary for the model. We can see that we have a total of 19909 charachters in our data and 27 unique characters. The characters are a-z (26 characters) plus the \"\\n\" (or newline character). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkkwRScxKWFb",
        "outputId": "f0c76d34-c82d-41fc-c83f-bc474934a811"
      },
      "source": [
        "data = open('dinos.txt', 'r').read()\r\n",
        "data= data.lower()\r\n",
        "chars = list(set(data))\r\n",
        "data_size, vocab_size = len(data), len(chars)\r\n",
        "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 19909 total characters and 27 unique characters in your data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYsTNq9bJfH1"
      },
      "source": [
        "Next, we split the data with respect to the \\n characters so as to obtain a a list of the names which will act as an input to our model. We add a \".\" at the end which will act as an EOS token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7BUBoQeACNC"
      },
      "source": [
        "names=data.split()\r\n",
        "names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAG4M3CtApks",
        "outputId": "50d7c41d-8a3c-4741-8f4c-8d0934780186"
      },
      "source": [
        "names = list(map(lambda s: s + '.', names))\r\n",
        "names[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aachenosaurus.',\n",
              " 'aardonyx.',\n",
              " 'abdallahsaurus.',\n",
              " 'abelisaurus.',\n",
              " 'abrictosaurus.',\n",
              " 'abrosaurus.',\n",
              " 'abydosaurus.',\n",
              " 'acanthopholis.',\n",
              " 'achelousaurus.',\n",
              " 'acheroraptor.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mguG18-KBKB"
      },
      "source": [
        "In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYHYOXwSKmLu"
      },
      "source": [
        "char_to_index = dict( (chr(i+96), i) for i in range(1,27))\r\n",
        "char_to_index[' '] = 0\r\n",
        "char_to_index['.'] = 27\r\n",
        "\r\n",
        "# Convert from index to character\r\n",
        "index_to_char = dict( (i, chr(i+96)) for i in range(1,27))\r\n",
        "index_to_char[0] = ' '\r\n",
        "index_to_char[27] = '.'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqBNOv_lKiVl"
      },
      "source": [
        "We create some variable to store the maximum length of a possible name, total number of names in the input data and the number of characters in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_oTF-HsA3-1"
      },
      "source": [
        "max_char = len(max(names, key=len))\r\n",
        "m = len(names)\r\n",
        "char_dim = len(char_to_index)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYM-Sv3CKxvZ"
      },
      "source": [
        "**TRAINING DATA** <BR>\r\n",
        "We create the training data set by initializing two zero matrices, one as the input and other as th eexpected output characters. For each of the m names in our dataset, we create a 2 dimensional matrix. Each matrix contains a row for each character in the name. (Note that there are always the same number of rows and if the name doesn't have enough characters to fill the whole matrix the remaining rows contain nothing.) Each of these rows represents one character and it is encoded as a one-hot vector. This means that it is a vector of zeros with a one only in the entry that corresponds to the character that is present.\r\n",
        "\r\n",
        "The output Y is the same as the input but translated by one unit. This means that the ith character in Y is the (i+1)th one in the actual name. This means that the network predicts the character that follows a given character in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzeZ8e8U_au6"
      },
      "source": [
        "X = np.zeros((m, max_char, char_dim))\r\n",
        "Y = np.zeros((m, max_char, char_dim))\r\n",
        "\r\n",
        "for i in range(m):\r\n",
        "    name = list(names[i])\r\n",
        "    for j in range(len(name)):\r\n",
        "        X[i, j, char_to_index[name[j]]] = 1\r\n",
        "        if j < len(name)-1:\r\n",
        "            Y[i, j, char_to_index[name[j+1]]] = 1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLt2HCjtMHbo"
      },
      "source": [
        "**MODEL WITH A LSTM LAYER**\r\n",
        "<BR>\r\n",
        "In the case of interest here we only consider one layer of recurrence, which we take to be LSTM with 128 units. We return the output of this layer and use it into a fully connected dense layer that converts the result of the LSTM layer into a vector of size char_dim using a softmax activation. We use categorical cross entropy as a cost function because of the softmax result and use Adam optimization. There is not really any useful metric to judge if the model does good so we will mostly just look at the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUd65JQ0K2am"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(LSTM(128, input_shape=(max_char, char_dim), return_sequences=True))\r\n",
        "model.add(Dense(char_dim, activation='softmax'))\r\n",
        "\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmsGWwf0Moj0"
      },
      "source": [
        "We create a function to generate new names. We input an empty character to the model to predict a random output for the first time step that will be further used by the next layer as its input. We normalize the probabilities of occurence of each character and then randomly pick one of the most probable characters. \".\" character will signify the end of the name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee6f5E6cMaA9"
      },
      "source": [
        "def make_name(model):\r\n",
        "    name = []\r\n",
        "    x = np.zeros((1, max_char, char_dim))\r\n",
        "    end = False\r\n",
        "    i = 0\r\n",
        "    \r\n",
        "    while end==False:\r\n",
        "        probs = list(model.predict(x)[0,i])\r\n",
        "        probs = probs / np.sum(probs)\r\n",
        "        index = np.random.choice(range(char_dim), p=probs)\r\n",
        "        if i == max_char-2:\r\n",
        "            character = '.'\r\n",
        "            end = True\r\n",
        "        else:\r\n",
        "            character = index_to_char[index]\r\n",
        "        name.append(character)\r\n",
        "        x[0, i+1, index] = 1\r\n",
        "        i += 1\r\n",
        "        if character == '.':\r\n",
        "            end = True\r\n",
        "    \r\n",
        "    print(''.join(name))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrkJkMKnNXTc"
      },
      "source": [
        " def generate_name_loop(epoch, _):  \r\n",
        "    if epoch % 25 == 0:\r\n",
        "        \r\n",
        "        print('Names generated after epoch %d:' % epoch)\r\n",
        "\r\n",
        "        for i in range(3):\r\n",
        "            make_name(model)\r\n",
        "        \r\n",
        "        print()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoSUYKxJNmTI"
      },
      "source": [
        "Now we want to use this function during the training to monitor how the generated names get better. To this end we create a function that will be given to the model when we fit it. We basically run the previous function a few times every 50 epochs and print the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izUDfDibNhq3"
      },
      "source": [
        "name_generator = LambdaCallback(on_epoch_end = generate_name_loop)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_ll4Xk6Nkm2",
        "outputId": "5f6a517c-ef37-40f0-83bc-c10a23e98e18"
      },
      "source": [
        "model.fit(X, Y, batch_size=64, epochs=300, callbacks=[name_generator], verbose=0)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Names generated after epoch 0:\n",
            "aeovda .\n",
            "mwoccblsu.\n",
            "bhpfm.\n",
            "\n",
            "Names generated after epoch 25:\n",
            "huriposaurus.\n",
            "wortisausus.\n",
            "burodusmucanatora.\n",
            "\n",
            "Names generated after epoch 50:\n",
            "ujiangonrathos.\n",
            "umarkasaurus.\n",
            ".\n",
            "\n",
            "Names generated after epoch 75:\n",
            "odonyx.\n",
            "kitsteptyryxaxa.\n",
            "ichixavosaurus.\n",
            "\n",
            "Names generated after epoch 100:\n",
            "pstoceracovdinakraimoseur.\n",
            "habrosaurus.\n",
            "chenosaurus.\n",
            "\n",
            "Names generated after epoch 125:\n",
            "ugraceltir.\n",
            "antarosaurus.\n",
            "zongyrannauran.\n",
            "\n",
            "Names generated after epoch 150:\n",
            "centsoepter.\n",
            "urakesaurus.\n",
            "rindan.\n",
            "\n",
            "Names generated after epoch 175:\n",
            "eorartia.\n",
            "jacerltops.\n",
            "ulansaurus.\n",
            "\n",
            "Names generated after epoch 200:\n",
            "yzhoullisaurus.\n",
            "icetaosaurus.\n",
            "uncanosaurus.\n",
            "\n",
            "Names generated after epoch 225:\n",
            "halg.\n",
            "ingosaurus.\n",
            "asthophaurus.\n",
            "\n",
            "Names generated after epoch 250:\n",
            "hangzosaurus.\n",
            "angwatia.\n",
            "hipposaurus.\n",
            "\n",
            "Names generated after epoch 275:\n",
            "ethorosaurus.\n",
            "elicrisaurus.\n",
            "ardosaurus.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb050cf4a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aFwIxyHNbad"
      },
      "source": [
        "**FINAL OUTPUT OF NAMES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CGR4p5-8PBc",
        "outputId": "a2281716-656c-4156-f5b7-f21f9ec1f758"
      },
      "source": [
        "for i in range(20):\r\n",
        "    make_name(model)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "oloroptor.\n",
            "rizalong.\n",
            "haypalichus.\n",
            "oththolia.\n",
            "untaratons.\n",
            "utahania.\n",
            "huylanvisaurus.\n",
            "ethaisanous.\n",
            "uncangodia.\n",
            "narstous.\n",
            "utahosaurus.\n",
            "opiseraptor.\n",
            "ixianosaurus.\n",
            "eneherpstes.\n",
            "quillithon.\n",
            "eninoraptor.\n",
            "therodontos.\n",
            "utahelosaurus.\n",
            "qijballodon.\n",
            "ormanoceratops.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}